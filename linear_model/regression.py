"""  @author Victor I. Afolabi  A.I. Engineer & Software developer  javafolabi@gmail.com  Created on 25 August, 2017 @ 8:15 PM.  Copyright © 2017. victor. All rights reserved."""import numpy as npclass LinearRegression(object):    """        Ordinary least squares Linear Regression.        Parameters        ----------        learning_rate : float, optional, default 1e-4            the learning rate which determines how the            model reaches convergence.            A small learning_rate may take longer to train,            while a large learning_rate may overshoot the local            minimal or diverge(never converge)        Attributes        ----------        W : array, shape (n_features, ) or (n_features, n_targets)            Estimated coefficients for the linear regression problem.            If multiple targets are passed during the fit (y 2D), this            is a 2D array of shape (n_targets, n_features), while if only            one target is passed, this is a 1D array of length n_features.        b : array, shape (n_targets,) or (1,) or empty            Bias, added after matrix multiply of weights and features. Also            referred to as y_intercept i.e where the regression line crosses            the y-axis. If the linear regression problem is under-determined            (the number of linearly independent rows of the training matrix is less            than its number of linearly independent columns), this is an empty            array. If the target vector passed during the fit is 1-dimensional,            this is a (1,) shape array.        Notes        -----        This is a naïve implementation of the LinearRegression algorithm.        A more sophisticated approach can be found using the        sklearn.linear_model.LinearRegression        """    def __init__(self, learning_rate=1e-4):        self.W = None        self.b = None        self.learning_rate = learning_rate    def fit(self, X, y, num_iter=1000):        """        Fit linear model.        Parameters        ----------        X : numpy array or sparse matrix of shape [n_samples,n_features]            Training data        y : numpy array of shape [n_samples, n_targets]            Target values        num_iter : int            Number of training iterations        Returns        -------        self : returns an instance of self.        """        weight_shape = (X.shape[1], y.shape[1])        bias_shape = (y.shape[1])        self.W = np.random.random(weight_shape)        self.b = np.random.random(bias_shape)        for _ in range(num_iter):            self.__gradient_descent(X, y)        return self    def predict(self, X):        """Predict using the linear model        Parameters        ----------        X : {array-like, sparse matrix}, shape = (n_samples, n_features)            Samples.        Returns        -------        C : array, shape = (n_samples,)            Returns predicted values.        """        return np.dot(X, self.W) + self.b    def score(self, X, y, sample_weight=None):        """Returns the mean accuracy on the given test data and labels.        In multi-label classification, this is the subset accuracy        which is a harsh metric since you require for each sample that        each label set be correctly predicted.        Parameters        ----------        X : array-like, shape = (n_samples, n_features)            Test samples.        y : array-like, shape = (n_samples) or (n_samples, n_outputs)            True labels for X.        sample_weight : array-like, shape = [n_samples], optional            Sample weights.        Returns        -------        score : float            Mean accuracy of self.predict(X) wrt. y.        """        from sklearn.metrics import accuracy_score        return accuracy_score(y, self.predict(X), sample_weight=sample_weight)    def error(self, X, y):        """Returns the mean error on the given test data and labels.            Parameters            ----------            X : array-like, shape = (n_samples, n_features)                Test samples.            y : array-like, shape = (n_samples) or (n_samples, n_outputs)                True labels for X.            Returns            -------            score : float                Mean accuracy of self.predict(X) wrt. y.        """        y_hat = self.predict(X)        total_error = np.square(np.subtract(y, y_hat))        return np.mean(total_error)    def __gradient_descent(self, X, y):        N = len(y)        W_gradient = np.sum(np.divide(-2 * (np.subtract(y, self.predict(X)) * X), N))        b_gradient = np.sum(np.divide(-2 * np.subtract(y, self.predict(X)), N))        self.W -= (self.learning_rate * W_gradient)        self.b -= (self.learning_rate * b_gradient)if __name__ == '__main__':    X, y = np.genfromtxt('../datasets/data.csv', delimiter=',', unpack=True)    X = X.reshape(-1, 1)    y = y.reshape(-1, 1)    from sklearn.model_selection import train_test_split    X_train, X_test, y_train, y_test = train_test_split(X, y)    # !- Using the model    clf = LinearRegression()    clf.fit(X_train, y_train)    # !- Evaluating the accuracy    err = clf.error(X_test, y_test)    print('Classification error = {:.02f}'.format(err))